{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1yyeiVXypOdRpfhunEihIOY7_d80gn4ky",
      "authorship_tag": "ABX9TyN+JkZCzdzHX+jP7CmfaJQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinNekouei/PyTorch_Binary-Prediction-of-Poisonous-Mushrooms/blob/main/Torch_binary_prediction_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Dataset**"
      ],
      "metadata": {
        "id": "6xog5znB3_bt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WNUCNUR-kOhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04353981-1a99-4c6a-e76f-1580f49f3e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/data"
      ],
      "metadata": {
        "id": "kxP1SbrFs1uN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/dataset/playground-series-s4e8_2.zip /content/data"
      ],
      "metadata": {
        "id": "OEVAVPV6kvFi",
        "outputId": "d68f729f-0dd7-4e61-f1e5-92195ac42787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/dataset/playground-series-s4e8_2.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/data/playground-series-s4e8_2.zip -d /content/data"
      ],
      "metadata": {
        "id": "NcDDVmvQs7t-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d21836-9176-472c-f678-2c3b5c286d44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/data/playground-series-s4e8_2.zip, /content/data/playground-series-s4e8_2.zip.zip or /content/data/playground-series-s4e8_2.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm /content/test.csv\n",
        "# !rm /content/train.csv\n",
        "# !rm /content/sample_submission.csv"
      ],
      "metadata": {
        "id": "BrctzKQttAm0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Library**"
      ],
      "metadata": {
        "id": "wZMvzMcl4Jfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import joblib  # For saving and loading the encoder"
      ],
      "metadata": {
        "id": "ux12uwaUtzC1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "RKJri4_6u-bo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yhp5K0_ATAOE",
        "outputId": "fd074f00-3af1-4c51-c8d7-8bb9b886d701"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step1: Data**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NlvdkgJqxzgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "sKsvCrKgyjci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Load Train set*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eaLnBlsK2_Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df= pd.read_csv('/content/data/train.csv')\n",
        "train_df= train_df.drop(columns=['id'])\n",
        "train_df"
      ],
      "metadata": {
        "id": "5i0hAaSQvTa3",
        "outputId": "9c7bf367-cb7a-40b3-e411-42798094b728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-253c2c5e4847>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df= pd.read_csv('/content/data/test.csv')\n",
        "test_df= test_df.drop(columns=['id'])\n",
        "test_df.insert(0, 'class', 'z')\n",
        "test_df.info()"
      ],
      "metadata": {
        "id": "s-puDMshbVvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "kbig6NBBt3uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *NaN*"
      ],
      "metadata": {
        "id": "TGds_O8X3Wxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isna().mean()"
      ],
      "metadata": {
        "id": "doVv4vGXvqc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.isna().mean()"
      ],
      "metadata": {
        "id": "y0M2Kji3bN-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_threshold= 0.85\n",
        "\n",
        "high_missing_train_columns= train_df.columns[train_df.isna().mean() > missing_threshold]\n",
        "high_missing_train_columns\n",
        "\n",
        "train_df= train_df.drop(columns= high_missing_train_columns)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "Fv0NuuIBvvvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_missing_test_columns= test_df.columns[test_df.isna().mean() > missing_threshold]\n",
        "print(high_missing_test_columns)\n",
        "\n",
        "test_df= test_df.drop(columns= high_missing_test_columns)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "nGiSr4NUbc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mode_values= train_df.mode()\n",
        "train_mode_values= train_mode_values.loc[0].to_dict()\n",
        "train_mode_values"
      ],
      "metadata": {
        "id": "XXaMAt1PxdGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mode_values= test_df.mode()\n",
        "test_mode_values= test_mode_values.loc[0].to_dict()\n",
        "test_mode_values"
      ],
      "metadata": {
        "id": "KfL9rFWhbtGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_median_values= train_df.median(numeric_only= True).to_dict()\n",
        "train_median_values"
      ],
      "metadata": {
        "id": "V8kDJGFwycVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_median_values= test_df.median(numeric_only= True).to_dict()\n",
        "test_median_values"
      ],
      "metadata": {
        "id": "7k5YE73Lb_F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in train_df:\n",
        "  if train_df[column].isna().any():\n",
        "    if train_df[column].dtype== 'object':\n",
        "      train_df[column]= train_df[column].fillna(train_mode_values[column])\n",
        "    else:\n",
        "      train_df[column]= train_df[column].fillna(train_median_values[column])\n",
        "\n",
        "train_df.isna().median()"
      ],
      "metadata": {
        "id": "FKRGV0_ZZ_1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in test_df:\n",
        "  if test_df[column].isna().any():\n",
        "    if test_df[column].dtype== 'object':\n",
        "      test_df[column]= test_df[column].fillna(test_mode_values[column])\n",
        "    else:\n",
        "      test_df[column]= test_df[column].fillna(test_median_values[column])\n",
        "\n",
        "test_df.isna().median()"
      ],
      "metadata": {
        "id": "zY_CupO8cG9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Split Dataframe to train and validation*"
      ],
      "metadata": {
        "id": "mreY_G_RkFXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples, num_features= train_df.shape\n",
        "num_features-= 1\n",
        "num_classes= len(train_df['class'].unique())\n",
        "print(f'num_sample: {num_samples}, num_features: {num_features}, num_classes: {num_classes}')"
      ],
      "metadata": {
        "id": "f8HWG3TRnGwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df= train_test_split(train_df, test_size=0.2, random_state=42, stratify= train_df['class'])\n",
        "print(f'train_df: {train_df.shape}, val_df: {val_df.shape}')"
      ],
      "metadata": {
        "id": "DHL6x0UOblfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Encoding*"
      ],
      "metadata": {
        "id": "eAenCkbcopup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_encoder= OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value= -1)\n",
        "cat_col_name= train_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(cat_col_name)\n",
        "\n",
        "train_df[cat_col_name]= ordinal_encoder.fit_transform(train_df[cat_col_name].astype(str))\n",
        "val_df[cat_col_name]= ordinal_encoder.transform(val_df[cat_col_name].astype(str))\n",
        "\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "FYczavgkk4wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[cat_col_name]= ordinal_encoder.transform(test_df[cat_col_name].astype(str))\n",
        "test_df[cat_col_name]\n",
        "\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "SapXP8EdwZYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Delete unused variable\n",
        "# del train_df\n",
        "\n",
        "# # Force  garbage collection\n",
        "# import gc\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "xqRTMyv8sbKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check current RAM usage\n",
        "import psutil\n",
        "ram_usage = psutil.virtual_memory()\n",
        "print(f\"Used RAM: {ram_usage.used / (1024 ** 3):.2f} GB\")\n",
        "print(f\"Available RAM: {ram_usage.available / (1024 ** 3):.2f} GB\")"
      ],
      "metadata": {
        "id": "ndWK-tIJu-U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Normalization*"
      ],
      "metadata": {
        "id": "hq2Ft3OTrZ2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaler= StandardScaler()\n",
        "# num_features_list= list(train_median_values.keys())\n",
        "# num_features_list\n",
        "\n",
        "# train_df[num_features_list]= scaler.fit_transform(train_df[num_features_list])\n",
        "# val_df[num_features_list]= scaler.transform(val_df[num_features_list])\n",
        "\n",
        "# test_df[num_features_list]= scaler.transform(test_df[num_features_list])\n",
        "# train_df.head(2)\n",
        "# test_df.head(2)"
      ],
      "metadata": {
        "id": "mLsfnslQu-4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *HiTorch*"
      ],
      "metadata": {
        "id": "jRfCgFOT5MwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.drop(columns=['class'], axis=1).values\n",
        "y_train= train_df['class'].values.reshape(-1,1)\n",
        "\n",
        "X_val= val_df.drop(columns=['class'], axis=1).values\n",
        "y_val= val_df['class'].values.reshape(-1, 1)\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "d3C0ae1gu6aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train= torch.tensor(X_train, dtype= torch.float32)\n",
        "y_train= torch.tensor(y_train, dtype= torch.float32)\n",
        "\n",
        "X_val= torch.tensor(X_val, dtype= torch.float32)\n",
        "y_val= torch.tensor(y_val, dtype= torch.float32)\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_train[1]"
      ],
      "metadata": {
        "id": "oY7DiHjF4Zns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test= test_df.drop(columns=['class'],axis=1).values\n",
        "y_test= test_df['class'].values.reshape(-1, 1)\n",
        "\n",
        "X_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "94fZ5H-2w3bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test= torch.tensor(X_test, dtype= torch.float32)\n",
        "y_test= torch.tensor(y_test, dtype= torch.float32)"
      ],
      "metadata": {
        "id": "9lvGSrINxLnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set= TensorDataset(X_train, y_train)\n",
        "val_set= TensorDataset(X_val, y_val)\n",
        "test_set= TensorDataset(X_test, y_test)\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "h1VL8l6S89C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader= DataLoader(train_set, batch_size=128, shuffle= True)\n",
        "val_loader= DataLoader(val_set, batch_size= 128)\n",
        "test_loader= DataLoader(test_set, batch_size= 128)"
      ],
      "metadata": {
        "id": "MORQDMprCqqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step2: Model**"
      ],
      "metadata": {
        "id": "blGkW9Psz5N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nf_hidden_layer1= 8 * num_features\n",
        "nf_hidden_layer2= 4 * num_features\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(in_features= num_features, out_features= nf_hidden_layer1),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features= nf_hidden_layer1, out_features= nf_hidden_layer2),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features= nf_hidden_layer2, out_features= num_features),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(num_features, num_classes-1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Z0CB9KvIz8om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step3: Loss function**"
      ],
      "metadata": {
        "id": "ihdVZwBUD3WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn= nn.BCELoss()\n",
        "loss_fn"
      ],
      "metadata": {
        "id": "mhx2QpGGGHgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step4: Optimizer**"
      ],
      "metadata": {
        "id": "6cfEKpvFGpoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt= torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "opt"
      ],
      "metadata": {
        "id": "9vc3w1rHGo-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step5: Train loop**"
      ],
      "metadata": {
        "id": "8eilUsCEHUGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matthews_correlation_coefficient(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Matthews Correlation Coefficient (MCC) on GPU using PyTorch.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Ground truth labels (binary, 0 or 1).\n",
        "        y_pred (torch.Tensor): Predicted labels (binary, 0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        mcc (torch.Tensor): MCC value.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    TP = ((y_true == 1) & (y_pred == 1)).sum().float()\n",
        "    TN = ((y_true == 0) & (y_pred == 0)).sum().float()\n",
        "    FP = ((y_true == 0) & (y_pred == 1)).sum().float()\n",
        "    FN = ((y_true == 1) & (y_pred == 0)).sum().float()\n",
        "\n",
        "    # Calculate numerator and denominator\n",
        "    numerator = (TP * TN) - (FP * FN)\n",
        "    denominator = torch.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
        "\n",
        "    # Handle division by zero\n",
        "    if denominator == 0:\n",
        "        return torch.tensor(0.0, device=y_true.device)\n",
        "    else:\n",
        "        return numerator / denominator"
      ],
      "metadata": {
        "id": "0XsO895og_CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss_val = 10000\n",
        "loss_train_hist, loss_val_hist, loss_test_hist =[], []\n",
        "acc_train_hist, acc_val_hist, acc_test_hist= [], []"
      ],
      "metadata": {
        "id": "2wolHbtEZXNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs= 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  mean_loss_train, mean_acc_train, train_mcc= 0, 0, 0\n",
        "  mean_loss_val, mean_acc_val, val_mcc= 0, 0, 0\n",
        "\n",
        "  for x_batch, y_batch in train_loader:\n",
        "\n",
        "    #GPU\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch= y_batch.to(device)\n",
        "\n",
        "    #model\n",
        "    y_hat= model(x_batch)\n",
        "\n",
        "    #loss\n",
        "    loss= loss_fn(y_hat, y_batch)\n",
        "\n",
        "    # gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    mean_loss_train += loss.item() * len(x_batch)\n",
        "    mean_acc_train += torch.sum(y_hat.round() == y_batch).item()\n",
        "    # train_mcc += matthews_correlation_coefficient(y_batch, y_hat.round())\n",
        "\n",
        "  mean_loss_train /= len(train_set)\n",
        "  mean_acc_train /= len(train_set)\n",
        "  # train_mcc /= len(train_set)\n",
        "  loss_train_hist.append(mean_loss_train)\n",
        "  acc_train_hist.append(mean_acc_train)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x_batch, y_batch in val_loader:\n",
        "\n",
        "      #GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch= y_batch.to(device)\n",
        "\n",
        "      y_hat= model(x_batch)\n",
        "\n",
        "      loss= loss_fn(y_hat, y_batch)\n",
        "\n",
        "      mean_loss_val += loss.item() * len(x_batch)\n",
        "      mean_acc_val += torch.sum(y_hat.round() == y_batch).item()\n",
        "      # val_mcc += matthews_correlation_coefficient(y_batch, y_hat.round())\n",
        "      # print(f'Matthews Correlation Coefficient: {val_mcc:.3f}')\n",
        "\n",
        "    mean_loss_val /= len(val_set)\n",
        "    mean_acc_val /= len(val_set)\n",
        "    # val_mcc /= len(val_set)\n",
        "    loss_val_hist.append(mean_loss_val)\n",
        "    acc_val_hist.append(mean_acc_val)\n",
        "\n",
        "  print(f'epoch[{epoch}]: '\n",
        "      f'Train_loss: {mean_loss_train:.3f} ,'\n",
        "      f'Train_acc: {mean_acc_train:.3f} ,'\n",
        "      # f'mcc: {train_mcc:.3f} '\n",
        "      f'val_loss: {mean_loss_val:.3f} ,'\n",
        "      f'val_acc: {mean_acc_val:.3f} ,'\n",
        "      # f'mcc: {val_mcc:.3f}'\n",
        "      )\n",
        "\n",
        "  if mean_loss_val < best_loss_val:\n",
        "    best_loss_val = mean_loss_val\n",
        "    print('model saved!')\n",
        "    print()\n",
        "    torch.save(model,'/content/drive/MyDrive/Projects/best_model.pt')"
      ],
      "metadata": {
        "id": "qgUuzj-RHTHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.arange(epochs), loss_train_hist)\n",
        "plt.plot(torch.arange(epochs), loss_val_hist)\n",
        "\n",
        "plt.legend([\"Train\", \"Valid\"]);"
      ],
      "metadata": {
        "id": "HS4TXCP-NOdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.arange(epochs), acc_train_hist)\n",
        "plt.plot(torch.arange(epochs), acc_val_hist)\n",
        "plt.legend([\"Train\", \"Valid\"]);"
      ],
      "metadata": {
        "id": "5ErXc8n1aNpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"best-model.pt\")"
      ],
      "metadata": {
        "id": "HqGEq_7CbXPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    y_hat = model(x_batch)\n",
        "    predictions.extend(y_hat.cpu().numpy())\n",
        "\n",
        "\n",
        "# Generate IDs starting from 3116945\n",
        "start_id = 3116945\n",
        "ids = list(range(start_id, start_id + len(predictions)))\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "predictions_df = pd.DataFrame({\n",
        "    'ID': ids,  # Add ID column\n",
        "    'Prediction': predictions  # Add prediction column\n",
        "})\n"
      ],
      "metadata": {
        "id": "Zbp57H0lyKDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast"
      ],
      "metadata": {
        "id": "G2882WE6rdUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df['Prediction'] = predictions_df['Prediction'].apply(lambda x: int(ast.literal_eval(x)[0]))\n",
        "predictions_df"
      ],
      "metadata": {
        "id": "1-MkNU1Mrvom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map= {0: 'e', 1: 'p'}\n",
        "\n",
        "predictions_df['Prediction']= predictions_df['Prediction'].apply(lambda x: label_map[x])\n",
        "predictions_df"
      ],
      "metadata": {
        "id": "c1WIFT9UvQHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df= predictions_df.rename(columns= {'ID': 'id', 'Prediction': 'class'})\n",
        "predictions_df"
      ],
      "metadata": {
        "id": "eHJQMPSGzgGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Excel\n",
        "predictions_df.to_csv('/content/drive/MyDrive/dataset/playground-series-s4e8_2.csv', index=False)\n",
        "\n",
        "print(\"Predictions with IDs saved to 'playground-series-s4e8_2.csv'\")"
      ],
      "metadata": {
        "id": "XI63N1W2r9Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYI7IBd5y5nP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}